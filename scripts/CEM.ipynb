{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca31385",
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing the libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(40) # suppress deprecation messages\n",
    "tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs  \n",
    "from math import floor, ceil\n",
    "import sklearn as sk\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Bidirectional, LSTM,Masking,Embedding\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, accuracy_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate,train_test_split,GridSearchCV\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as pyplot\n",
    "import seaborn as sns\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638b14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT HERE FOR OTHER COURSES\n",
    "data_path = '/Volumes/MOOC/flipped/apr21-feature-mooc-flipped/'\n",
    "week_type = 'eq_week'\n",
    "feature_types = ['akpinar_et_al', 'boroujeni_et_al', \n",
    "                 'chen_cui', 'he_et_al', 'lalle_conati','lemay_doleck', \n",
    "                 'marras_et_al', 'mbouzao_et_al', 'mubarak_et_al', 'wan_et_al']\n",
    "course = 'epfl_algebrelineaire'\n",
    "marras_et_al_id = feature_types.index('marras_et_al')\n",
    "akpinar_et_al_id = feature_types.index('akpinar_et_al')\n",
    "remove_obvious = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c3752",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca74f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillNaN(feature):\n",
    "    shape = feature.shape\n",
    "    feature_min = np.nanmin(feature.reshape(-1,shape[2]),axis=0)\n",
    "    feature = feature.reshape(-1,shape[2])\n",
    "    inds = np.where(np.isnan(feature))\n",
    "    feature[inds] = np.take(feature_min.reshape(-1), inds[1])\n",
    "    feature = feature.reshape(shape)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd5a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 14, 347)\n",
      "(2996, 347)\n",
      "(214, 14, 3)\n",
      "(2996, 3)\n",
      "(214, 14, 13)\n",
      "(2996, 13)\n",
      "(214, 14, 3)\n",
      "(2996, 3)\n",
      "(214, 14, 22)\n",
      "(2996, 22)\n",
      "(214, 14, 10)\n",
      "(2996, 10)\n",
      "(214, 14, 12)\n",
      "(2996, 12)\n",
      "(214, 14, 3)\n",
      "(2996, 3)\n",
      "(214, 14, 13)\n",
      "(2996, 13)\n",
      "(214, 14, 14)\n",
      "(2996, 14)\n",
      "course:  epfl_algebrelineaire\n",
      "week_type:  eq_week\n",
      "feature_type:  ['akpinar_et_al', 'boroujeni_et_al', 'chen_cui', 'he_et_al', 'lalle_conati', 'lemay_doleck', 'marras_et_al', 'mbouzao_et_al', 'mubarak_et_al', 'wan_et_al']\n"
     ]
    }
   ],
   "source": [
    "# Loading the features\n",
    "feature_list = {}\n",
    "\n",
    "feature_type_list = []\n",
    "for feature_type in feature_types:\n",
    "\n",
    "    filepath = data_path + week_type + '-' + feature_type + '-' + course\n",
    "    feature_current = np.load(filepath+'/feature_values.npz')['feature_values']\n",
    "    print(feature_current.shape)\n",
    "    feature_norm = feature_current.reshape(-1,feature_current.shape[2] )\n",
    "    print(feature_norm.shape)\n",
    "    feature_type_list.append(pd.DataFrame(feature_norm))\n",
    "feature_list[course] = feature_type_list\n",
    "\n",
    "print('course: ', course)\n",
    "print('week_type: ', week_type)\n",
    "print('feature_type: ', feature_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80b0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0,   1,   2,   3,  41, 103, 118, 132, 150, 217, 243, 341, 346]),)\n",
      "akpinar_et_al 13\n",
      "boroujeni_et_al 9\n",
      "chen_cui 13\n",
      "he_et_al 3\n",
      "lalle_conati 22\n",
      "lemay_doleck 10\n",
      "marras_et_al 12\n",
      "mbouzao_et_al 3\n",
      "mubarak_et_al 14\n",
      "wan_et_al 14\n"
     ]
    }
   ],
   "source": [
    "# Loading feature names\n",
    "feature_names= dict()\n",
    "\n",
    "for feature_type in feature_types:\n",
    "    \n",
    "    filepath = data_path + week_type + '-' + feature_type + '-' + course + '/settings.txt'\n",
    "    file = open(filepath, \"r\")\n",
    "    contents = file.read()\n",
    "    dictionary = ast.literal_eval(contents)\n",
    "    file.close()\n",
    "    \n",
    "    feature_type_name = dictionary['feature_names']\n",
    "    \n",
    "    if feature_type == 'akpinar_et_al':\n",
    "        feature_type_name = [clean_akp_name(x) for x in feature_type_name]\n",
    "        akp_mask = np.where(np.isin(feature_type_name, \n",
    "                 [\"TotalClicks\", \"NumberSessions\", \"Time-video-sum\", \"Time-problem-sum\",\n",
    "                  'problem.check-problem.check-problem.check', \n",
    "                  'problem.check-problem.check-video.load', \n",
    "                  'video.play-video.play-video.play',\n",
    "                  'video.play-video.pause-video.load',\n",
    "                  'video.play-problem.check-problem.check',\n",
    "                  'video.play-video.stop-video.play',\n",
    "                  'video.pause-video.speedchange-video.play',\n",
    "                  'video.stop-video.play-video.seek',\n",
    "                  'video.stop-problem.check-video.load']))\n",
    "        print(akp_mask)\n",
    "        feature_type_name = list(np.array(feature_type_name)[akp_mask[0]])\n",
    "        feature_list[course][akpinar_et_al_id] = feature_list[course][akpinar_et_al_id][akp_mask[0]]\n",
    "        \n",
    "    feature_names[feature_type] = feature_type_name\n",
    "    print(feature_type, len(feature_type_name))\n",
    "\n",
    "if remove_obvious: \n",
    "    # drop 'student shape', 'competency strength', 'competency alignment' in marras at al\n",
    "    \n",
    "    mask = np.where(np.isin(feature_names['marras_et_al'], \n",
    "                 ['StudentShape', 'CompetencyStrength', 'CompetencyAlignment']))\n",
    "    \n",
    "    new_marras = np.delete(np.array(feature_names['marras_et_al']), mask[0])\n",
    "    feature_names['marras_et_al'] = new_marras\n",
    "    \n",
    "    new_features = feature_list[course][marras_et_al_id].drop(mask[0], axis=1)\n",
    "    feature_list[course][marras_et_al_id] = new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6243d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat feature names\n",
    "# ex: time_sessions_<function sum at 0x7f3bd02cc9d0> -> time_sessions_sum\n",
    "def clean_name(feature):\n",
    "    id = feature.find('<')\n",
    "    if id==-1:\n",
    "        return feature\n",
    "    fct = feature[id+9:id+14].strip()\n",
    "    return feature[0:id]+fct\n",
    "\n",
    "\n",
    "for k in feature_names.keys():\n",
    "    cleaned = [clean_name(x) for x in feature_names[k]]\n",
    "    feature_names[k] = cleaned\n",
    "\n",
    "def clean_akp_name(feature):\n",
    "    feature = feature.lower()\n",
    "    if feature.find(\"(\")!=-1:\n",
    "        feature = feature[1:-1]\n",
    "        feature = feature.replace(', ', '-')\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0290e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(course):\n",
    "    feature_type = \"boroujeni_et_al\"\n",
    "    filepath = data_path + week_type + '-' + feature_type + '-' + course + '/feature_labels.csv'\n",
    "    labels = pd.read_csv(filepath)['label-pass-fail']\n",
    "    labels[labels.shape[0]] = 1\n",
    "    return labels.values\n",
    "\n",
    "def load_features(course):\n",
    "    feature_list = []\n",
    "    selected_features = []\n",
    "    total_features = set()\n",
    "    num_weeks = 0\n",
    "    num_features = 0\n",
    "    for i,feature_type in enumerate(feature_types):\n",
    "        filepath = data_path + week_type + '-' + feature_type + '-' + course \n",
    "        feature_current = np.load(filepath+'/feature_values.npz')['feature_values']\n",
    "        \n",
    "        shape = feature_current.shape\n",
    "#         print(shape)\n",
    "\n",
    "        if remove_obvious and feature_type=='marras_et_al':\n",
    "            feature_current = np.delete(feature_current, mask[0], axis=2)\n",
    "        \n",
    "        if feature_type=='akpinar_et_al':\n",
    "            akp_mask_dl = np.delete(list(range(shape[2])), akp_mask[0])\n",
    "            feature_current = np.delete(feature_current, akp_mask_dl, axis=2)\n",
    "        \n",
    "        shape = feature_current.shape\n",
    "        print(shape)\n",
    "        if i==0:\n",
    "            num_weeks = shape[1]\n",
    "            \n",
    "        selected = np.arange(shape[2])\n",
    "        # drop existed features\n",
    "        exist_mask = []\n",
    "        for i, name in enumerate(feature_names[feature_type]):\n",
    "            if name in total_features:\n",
    "                exist_mask.append(i)\n",
    "            else:\n",
    "                total_features.add(name)\n",
    "        feature_current = np.delete(feature_current, exist_mask, axis=2)\n",
    "        selected = np.delete(selected, exist_mask)\n",
    "        \n",
    "        nonNaN = (shape[0]*shape[1] - np.isnan(feature_current.reshape(-1,feature_current.shape[2])).sum(axis=0) > 0)\n",
    "        feature_current = feature_current[:,:,nonNaN]\n",
    "        selected = selected[nonNaN]\n",
    "        feature_current = fillNaN(feature_current)\n",
    "        nonZero = (abs(feature_current.reshape(-1,feature_current.shape[2])).sum(axis=0)>0)\n",
    "        selected = selected[nonZero]\n",
    "        feature_current = feature_current[:,:,nonZero]\n",
    "#         print(len(feature_names[feature_type]), selected)\n",
    "        selected_features.append(np.array(feature_names[feature_type])[[selected]])\n",
    "        num_features += len(np.array(feature_names[feature_type])[[selected]])\n",
    "\n",
    "\n",
    "        ##### Normalization with min-max. Added the artifical 1.001 max row for solving the same min max problem\n",
    "        ##### for features with max=0 I added 1 instead of 1.001 of maximum\n",
    "\n",
    "        features_min = feature_current.min(axis=0).reshape(-1)\n",
    "        features_max = feature_current.max(axis=0)\n",
    "        features_max = np.where(features_max==0,np.ones(features_max.shape),features_max)\n",
    "        max_instance = 1.001*features_max\n",
    "        feature_current = np.vstack([feature_current,max_instance.reshape((1,)+max_instance.shape)])\n",
    "        features_max = features_max.reshape(-1)\n",
    "        feature_norm = (feature_current.reshape(shape[0]+1,-1)-features_min)/(1.001*features_max-features_min)\n",
    "        feature_current = feature_norm.reshape(-1,feature_current.shape[1],feature_current.shape[2] )\n",
    "\n",
    "        feature_list.append(feature_current)\n",
    "        \n",
    "    features = np.concatenate(feature_list, axis=2)\n",
    "    features_min = features.min(axis=0).reshape(-1)\n",
    "    features_max = features.max(axis=0)\n",
    "    features = features.reshape(features.shape[0],-1)\n",
    "#     features = pd.DataFrame(features)\n",
    "    \n",
    "    SHAPE = features.shape\n",
    "    # print(np.isnan(features[0,0,-1]))\n",
    "    print(features.shape)\n",
    "    print('course: ', course)\n",
    "    print('week_type: ', week_type)\n",
    "    print('feature_type: ', feature_types)\n",
    "    print(selected_features)\n",
    "    return features, features_min, features_max, selected_features, num_weeks, num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee36a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load_labels(course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd2a7178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 14, 13)\n",
      "(214, 14, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/16/kn9v95ss6kx75q2kjs2mn9jm0000gn/T/ipykernel_68269/3089336469.py:52: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  selected_features.append(np.array(feature_names[feature_type])[[selected]])\n",
      "/var/folders/16/kn9v95ss6kx75q2kjs2mn9jm0000gn/T/ipykernel_68269/3089336469.py:53: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  num_features += len(np.array(feature_names[feature_type])[[selected]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214, 14, 13)\n",
      "(214, 14, 3)\n",
      "(214, 14, 22)\n",
      "(214, 14, 10)\n",
      "(214, 14, 9)\n",
      "(214, 14, 3)\n",
      "(214, 14, 13)\n",
      "(214, 14, 14)\n",
      "(215, 1148)\n",
      "course:  epfl_algebrelineaire\n",
      "week_type:  eq_week\n",
      "feature_type:  ['akpinar_et_al', 'boroujeni_et_al', 'chen_cui', 'he_et_al', 'lalle_conati', 'lemay_doleck', 'marras_et_al', 'mbouzao_et_al', 'mubarak_et_al', 'wan_et_al']\n",
      "[array(['TotalClicks', 'NumberSessions', 'Time-video-sum',\n",
      "       'Time-problem-sum', 'video.pause-video.speedchange-video.play',\n",
      "       'video.play-video.pause-video.load',\n",
      "       'video.play-video.play-video.play',\n",
      "       'video.play-video.stop-video.play',\n",
      "       'video.play-problem.check-problem.check',\n",
      "       'video.stop-video.play-video.seek',\n",
      "       'video.stop-problem.check-video.load',\n",
      "       'problem.check-problem.check-video.load',\n",
      "       'problem.check-problem.check-problem.check'], dtype='<U41'), array(['RegPeakTime-m1', 'RegPeriodicity-m1', 'DelayLecture'],\n",
      "      dtype='<U17'), array(['TimeSessions-sum', 'TimeSessions-mean', 'TimeBetweenSessions-std',\n",
      "       'TimeSessions-std', 'TotalClicks-weekday', 'TotalClicks-weekend',\n",
      "       'RatioClicksWeekendDay'], dtype='<U23'), array(['AttendanceRate', 'UtilizationRate', 'WatchingRatio'], dtype='<U15'), array(['TotalClicks-video.load', 'WeeklyProp-watched-mean',\n",
      "       'WeeklyProp-replayed-mean', 'WeeklyProp-interrupted-mean',\n",
      "       'FrequencyEvent-video', 'FrequencyEvent-video.load',\n",
      "       'FrequencyEvent-video.play', 'FrequencyEvent-video.pause',\n",
      "       'FrequencyEvent-video.stop', 'FrequencyEvent-video.seekbackward',\n",
      "       'FrequencyEvent-video.seekforward', 'SeekLength-mean',\n",
      "       'SeekLength-std', 'PauseDuration-mean', 'PauseDuration-std',\n",
      "       'TimeSpeedingUp-mean', 'TimeSpeedingUp-std'], dtype='<U33'), array(['FractionSpent-video.play', 'FractionSpent-video.play-completed',\n",
      "       'FractionSpent-video.play-played',\n",
      "       'FrequencyEvent-video.pause-total', 'FractionSpent-video.pause',\n",
      "       'SpeedPlayback-mean', 'SpeedPlayback-std',\n",
      "       'FrequencyEvent-video.seekbackward-total',\n",
      "       'FrequencyEvent-video.seekforward-total',\n",
      "       'CountUniqueElement-video'], dtype='<U39'), array(['CompetencyCoverage', 'CompetencyAnticipation', 'ContentAlignment',\n",
      "       'ContentCoverage', 'ContentAnticipation', 'StudentSpeed',\n",
      "       'StudentActiveness', 'StudentThoughtfulness',\n",
      "       'StudentWeeklyActiveness'], dtype='<U23'), array(['WatchingIndex'], dtype='<U15'), array(['FrequencyEvent-video.play-played',\n",
      "       'FrequencyEvent-video.play-relative',\n",
      "       'FrequencyEvent-video.pause-relative',\n",
      "       'FractionSpent-video.seek-time-backward',\n",
      "       'FractionSpent-video.seek-time-forward',\n",
      "       'FrequencyEvent-video.seelforward-total',\n",
      "       'FrequencyEvent-video.load-relative'], dtype='<U39'), array(['NumberSubmissions-distinct', 'NumberSubmissions',\n",
      "       'NumberSubmissions-distinct_correct', 'NumberSubmissions-avg',\n",
      "       'NumberSubmissions-avg_time', 'ObsDurationProblem',\n",
      "       'NumberSubmissions-perc_correct', 'TimeSolveProblem',\n",
      "       'ObsDurationProblem-var', 'ObsDurationProblem-max',\n",
      "       'TimeSessions-length', 'NumberSubmissions-correct'], dtype='<U34')]\n"
     ]
    }
   ],
   "source": [
    "features, features_min, features_max, selected_features, num_weeks, num_features = load_features(course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b74a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = dict()\n",
    "for i, feature_type in enumerate(feature_types):\n",
    "    feature_dict[feature_type] = list(selected_features[i])\n",
    "    \n",
    "selected_features = feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb44c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names= []\n",
    "final_features = []\n",
    "for feature_type in feature_types:\n",
    "    [final_features.append(x) for x in selected_features[feature_type]]\n",
    "for i in np.arange(num_weeks):\n",
    "    feature_type_name_with_weeks = [(x+'_InWeek'+str(i+1)) for x in final_features]\n",
    "    feature_names.append(feature_type_name_with_weeks)\n",
    "feature_names = np.concatenate(feature_names, axis=0)\n",
    "feature_names = feature_names.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26533fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=np.concatenate(((1-y).reshape(-1,1),y.reshape(-1,1)),axis=1)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "986b4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.DataFrame(features, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4445ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_f = list(selected_features.values())\n",
    "num_features = len([feature for feature_group in s_f for feature in feature_group])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6b9c6",
   "metadata": {},
   "source": [
    "## Model\n",
    "A new model has to be trained for CEM, since it needs a target variable of a different shape (n_instances, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bff5cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_lstm(x_train, y_train, x_test, y_test, x_val, y_val, week_type, feature_types, course,n_weeks,n_features, num_epochs=100):\n",
    "    n_dims = x_train.shape[0]\n",
    "    look_back = 3\n",
    "    # LSTM\n",
    "    # define model\n",
    "    lstm = Sequential()\n",
    "    ###########Reshape layer################\n",
    "    lstm.add(tf.keras.layers.Reshape((n_weeks, n_features), input_shape=(n_weeks*n_features,)))\n",
    "    ##########deleting the 1.001 max row added###########\n",
    "    lstm.add(Masking(mask_value = 1))\n",
    "    lstm.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    lstm.add(Bidirectional(LSTM(32)))\n",
    "    # Add a sigmoid Dense layer with 1 units.\n",
    "    lstm.add(Dense(2, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # fit the model\n",
    "    history = lstm.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=num_epochs, batch_size=32, verbose=1)\n",
    "    # evaluate the model\n",
    "    y_pred = lstm.predict(x_test)\n",
    "    print(y_pred.shape)\n",
    "    y_pred = np.array([1 if y >= 0.5 else 0 for y in y_pred[:,1]])\n",
    "    print(y_pred.shape)\n",
    "    y_pred = np.concatenate(((1-y_pred).reshape(-1,1),y_pred.reshape(-1,1)),axis=1)\n",
    "    print(y_pred.shape)\n",
    "    # evaluate the model\n",
    "    model_params = {'model': 'LSTM-bi', \n",
    "                    'epochs': num_epochs, \n",
    "                    'batch_size': 32, \n",
    "                    'loss': 'binary_cross_entropy'}\n",
    "    scores = evaluate(None, x_test, y_test, week_type, feature_types, course, y_pred=y_pred, model_name=\"TF-LSTM-bi\", model_params=model_params)\n",
    "    lstm.save('../models/lstm_bi_'+course+'_cem')\n",
    "    return history, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb34c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, file_name):\n",
    "    # plot loss during training\n",
    "    pyplot.figure(0)\n",
    "    pyplot.title('Loss ' + file_name)\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.xlabel(\"epoch\")\n",
    "    pyplot.ylabel(\"loss\")\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig(file_name + \"_loss.png\")\n",
    "    # plot accuracy during training\n",
    "    pyplot.figure(1)\n",
    "    pyplot.title('Accuracy ' + file_name)\n",
    "    pyplot.plot(history.history['acc'], label='train')\n",
    "    pyplot.plot(history.history['val_acc'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.xlabel(\"epoch\")\n",
    "    pyplot.ylabel(\"accuracy\")\n",
    "    pyplot.savefig(file_name + \"_acc.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0327eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_test, y_test, week_type, feature_type, course, model_name=None, model_params=None, y_pred=None):\n",
    "    scores={}\n",
    "    y_test=y_test[:,1]\n",
    "    y_pred=y_pred[:,1]\n",
    "    scores['test_acc'] = accuracy_score(y_test, y_pred)\n",
    "    scores['test_bac'] = balanced_accuracy_score(y_test, y_pred)\n",
    "    scores['test_prec'] = precision_score(y_test, y_pred)\n",
    "    scores['test_rec'] = recall_score(y_test, y_pred)\n",
    "    scores['test_f1'] = f1_score(y_test, y_pred)\n",
    "    scores['test_auc'] = roc_auc_score(y_test, y_pred)\n",
    "    scores['feature_type'] = feature_type\n",
    "    scores['week_type'] = week_type\n",
    "    scores['course'] = course\n",
    "    scores['data_balance'] = sum(y)/len(y)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd6e58a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215, 1148)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(215, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(features.shape)\n",
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ebe4313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de6cdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172, 1148) (21, 1148) (22, 1148)\n",
      "(172, 2) (21, 2) (22, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size=0.8\n",
    "x_train, x_rem, y_train, y_rem = train_test_split(features, labels, train_size=train_size, random_state=25)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_rem, y_rem, train_size=0.5, random_state=25)\n",
    "print(x_train.shape,x_test.shape,x_val.shape)\n",
    "print(y_train.shape,y_test.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c113fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_timestamp = str(time.time())[:-2]\n",
    "# model=bidirectional_lstm\n",
    "# print(model.__name__)\n",
    "# history, scores = model(x_train, y_train, x_test, y_test, x_val, y_val,week_type, feature_types, course, \n",
    "#                         num_epochs=10, n_weeks=num_weeks, n_features=num_features)\n",
    "# print(\"{:<15} {:<8} \".format('metric','value'))\n",
    "# for ke, v in scores.items():\n",
    "#     if isinstance(v, float):\n",
    "#         v=round(v, 4)\n",
    "#     if ke!=\"feature_type\":\n",
    "#         print(\"{:<15} {:<8} \".format(ke, v))\n",
    "# run_name = model.__name__ + \"_\" + course + \"_\" + current_timestamp\n",
    "# plot_history(history,run_name)\n",
    "# print(run_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da7e25",
   "metadata": {},
   "source": [
    "## Explainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bcef9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skye.du/opt/anaconda3/envs/ML4ED/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import alibi\n",
    "from alibi.explainers import CEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecd2d34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-09 09:09:55.349920: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "bilstm = load_model('../models/lstm_bi_'+course+'_cem')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ae7a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pn_all(num_instances,features,feature_names):\n",
    "    mode = 'PN'  # 'PN' (pertinent negative) or 'PP' (pertinent positive)\n",
    "    shape = (1,) + features.shape[1:]  # instance shape\n",
    "    kappa = 0.  # minimum difference needed between the prediction probability for the perturbed instance on the\n",
    "              # class predicted by the original instance and the max probability on the other classes \n",
    "              # in order for the first loss term to be minimized\n",
    "    beta = .1  # weight of the L1 loss term\n",
    "    gamma = 100  # weight of the optional auto-encoder loss term\n",
    "    c_init = 1.  # initial weight c of the loss term encouraging to predict a different class (PN) or \n",
    "                # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "    c_steps = 10  # nb of updates for c\n",
    "    max_iterations = 1000  # nb of iterations per value of c\n",
    "    feature_range = (features.min(axis=0),features.max(axis=0)) # feature range for the perturbed instance\n",
    "    clip = (-1000.,1000.)  # gradient clipping\n",
    "    lr = 1e-2  # initial learning rate\n",
    "    no_info_val = -1. # a value, float or feature-wise, which can be seen as containing no info to make a prediction\n",
    "                    # perturbations towards this value means removing features, and away means adding features\n",
    "                    # for our MNIST images, the background (-0.5) is the least informative, \n",
    "                    # so positive/negative perturbations imply adding/removing features\n",
    "    cem = CEM(bilstm, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range, \n",
    "    gamma = gamma, ae_model=None, max_iterations=max_iterations, \n",
    "    c_init = c_init, c_steps=c_steps, learning_rate_init=lr, clip=clip, no_info_val=no_info_val)\n",
    "    changes=[]\n",
    "    explanations = []\n",
    "    final_num_instances = []\n",
    "    for i in num_instances:\n",
    "        try:\n",
    "            X = features[i].reshape((1,) + features[0].shape)\n",
    "            explanation = cem.explain(X)\n",
    "            change = explanation.PN-X\n",
    "            print(f'counterfactuals generated for instance {i}')\n",
    "            changes.append(change)\n",
    "            explanations.append(explanation)\n",
    "            final_num_instances.append(i)\n",
    "        except TypeError:\n",
    "            print(f'Error occured for instance {i}')\n",
    "            print(change)\n",
    "    return explanations, changes, final_num_instances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b046452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_all(num_instances,features,feature_names):\n",
    "    mode = 'PP'  # 'PN' (pertinent negative) or 'PP' (pertinent positive)\n",
    "    shape = (1,) + features.shape[1:]  # instance shape\n",
    "    kappa = 0.  # minimum difference needed between the prediction probability for the perturbed instance on the\n",
    "              # class predicted by the original instance and the max probability on the other classes \n",
    "              # in order for the first loss term to be minimized\n",
    "    beta = .1  # weight of the L1 loss term\n",
    "    gamma = 100  # weight of the optional auto-encoder loss term\n",
    "    c_init = 1.  # initial weight c of the loss term encouraging to predict a different class (PN) or \n",
    "                # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "    c_steps = 10  # nb of updates for c\n",
    "    max_iterations = 1000  # nb of iterations per value of c\n",
    "    feature_range = (features.min(axis=0),features.max(axis=0)) # feature range for the perturbed instance\n",
    "    clip = (-1000.,1000.)  # gradient clipping\n",
    "    lr = 1e-2  # initial learning rate\n",
    "    no_info_val = -1. # a value, float or feature-wise, which can be seen as containing no info to make a prediction\n",
    "                    # perturbations towards this value means removing features, and away means adding features\n",
    "                    # for our MNIST images, the background (-0.5) is the least informative, \n",
    "                    # so positive/negative perturbations imply adding/removing features\n",
    "    cem = CEM(bilstm, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range, \n",
    "    gamma = gamma, ae_model=None, max_iterations=max_iterations, \n",
    "    c_init = c_init, c_steps=c_steps, learning_rate_init=lr, clip=clip, no_info_val=no_info_val)\n",
    "    changes=[]\n",
    "    for i in num_instances:\n",
    "        try:\n",
    "            X = features[i].reshape((1,) + features[0].shape)\n",
    "            explanation = cem.explain(X)\n",
    "            change = explanation.PP-X\n",
    "            print(f'counterfactuals generated for instance {i}')\n",
    "            changes.append(change)\n",
    "        except TypeError:\n",
    "            print(f'Error occured for instance {i}')\n",
    "            print(change)\n",
    "    return changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e646c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances = np.load('uniform_data/uniform_'+course+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea158448",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print('start time', t1)\n",
    "explanation, changes, final_num_instances = pn_all(num_instances,features,feature_names)\n",
    "t2 = time.time()\n",
    "print('end time', t2)\n",
    "print(f'time taken: {(t2-t1)/60.0} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db5c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_instances = features[final_num_instances]\n",
    "\n",
    "path = 'uniform_eq_results/CEM/'+course\n",
    "if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "np.save('uniform_eq_results/CEM/'+course+'/changes_pn', np.array(changes).reshape(len(final_num_instances),-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "pns = np.array([explanation[i].PN for i in range(len(explanation))]).reshape(len(final_num_instances),-1)\n",
    "np.save('uniform_eq_results/CEM/'+course+'/pns', pns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957183d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('uniform_eq_results/CEM/'+course+'/instances', final_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds = pd.DataFrame(features, columns=feature_names).describe()\n",
    "sds = sds.loc[:,~sds.columns.duplicated()]\n",
    "sds = sds.loc['std',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(changes).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc516813",
   "metadata": {},
   "outputs": [],
   "source": [
    "sds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7274761",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = pd.DataFrame(np.array(changes).reshape(len(final_num_instances),-1), columns=feature_names)\n",
    "diffs = diffs.loc[:,~diffs.columns.duplicated()]\n",
    "\n",
    "for col in diffs.columns:\n",
    "    diffs[col] = np.abs(diffs[col]*(sds[col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12691bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs.insert(0, 'exp_num', final_num_instances)\n",
    "diffs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs.to_csv('uniform_eq_results/CEM/'+course+'/importances.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a6aa06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
